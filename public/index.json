[{"content":"Starting last week I felt like I was ready to jump into attempting to work off the knowledge I had built up doing the examples previously and attempt the main goal; to create a model that was capable of recognising some of the Japanese Hiragana character set.\nProblem - Input Data! My initial problem that I had been playing with was the idea of a training data set - where would I get a good set of training data from?\nLuckily this was no where near as significant a problem as I had perceived it to be. The ETL Character Database is database of 1.2 billion characters - numerals, symbols, alphabetic and\u0026hellip; Japanese characters! The database I needed access to was the eighth database, ETL-8, which contained many written examples of the hiragana character set.\nI was concentrating on the first 5 characters of the Hiragana character set - あいうえお, or rather A I U E O. You may recognise these as they are there the exact same set of characters we use in English as vowels, just in a different order. These characters presented a good challenge as there are similar looking characters in the mix, like う/U and え/E. These look somewhat similar, similar enough it could confuse a trained model\u0026hellip; potentially.\nIn total this dataset provided me with 160 images per character, so a total of 800 images - an amazing dataset for something I previously had no idea how to solve!\nSolution - Train the model! With my input dataset issues solved I set off to train a model to use this dataset.\nInitially I was going to search for the most appropriate model to use and research if there were character-orientated recognition models already out there but then the idea hit me - characters like hiragana are drawings essentially. Why not just use the model from the Building a Pictionary-style game with AWS DeepLens example I did previously and change the training dataset?\nSo instead of going for all out perfection first round I did just the above. I created a new Jupyter notebook, transferred the required parts over to this new notebook and after failing multiple times due to menial syntactical mistakes I reach the point of testing the model I had created - with good results!\nThe excluded testing dataset passed with very high confidence! Things were looking great so far.\nThe real test has arrived - will a AWS DeeLens deployed model work as well as it did in testing? Deploying a functioning Lambda function to work with the AWS IoT output the AWS DeepLens was producing proved difficult beyond logic and is still unsolved - for some reason I cannot get any function that is using Python 3 (in particular the supported Python 3.7) to work. More on this another day, however long story short I reverted back to using Python 2.7 and strived to move forward with my depreciated runtime!\nI printed out a computer-generated set of A I U E O to post up on my wall to test my model. Below are the results;\nFor a model trained with a pretty vanilla setup these are impressive results! Admittedly the above results are for perfectly shaped computer-generated characters.\nWhen I use my own human-written characters the results are not as amazing. Still correct, but with a much lower level of confidence;\nNext Steps! Whilst this was a successful attempt and I\u0026rsquo;m happy with the results many improvements could be made;\n All ELT Database character data is scanned from paper and is therefore presented facing straight at the character, so to speak. It\u0026rsquo;s like we\u0026rsquo;re looking straight forward at the characters. This bias presents itself when using the AWS DeepLens - predictions are great when the camera is faced directly towards the character in question. When the camera moves slightly to the left or right however accuracy falls off quickly. Discovering how to overcome this weakness in my dataset will be interesting! I am currently using the same model as the Pictionary example; a image classification model. I am still wondering if there\u0026rsquo;s a more appropriate model out there. Python 3 is the bane of my existence when using the AWS DeepLens. Will I be able to overcome the compatibility issues? Time will tell!  ","permalink":"http://dev.drentin.au/posts/is-it-a-yes/","summary":"Starting last week I felt like I was ready to jump into attempting to work off the knowledge I had built up doing the examples previously and attempt the main goal; to create a model that was capable of recognising some of the Japanese Hiragana character set.\nProblem - Input Data! My initial problem that I had been playing with was the idea of a training data set - where would I get a good set of training data from?","title":"ML with a AWS DeepLens: Is it あ? Yes it is!"},{"content":"What we all came here for\u0026hellip; SageMaker! I was excited to get stuck into the Advanced recipe - Build a custom ML model to sort trash as this started getting into the parts I wanted to know more about; how to get a basic model trained in SageMaker and then deploy it to the DeepLens device.\nStep 1 - Train! Luckily in this example they include a number of sample images, quite a decent set really with over 500 images in total, seperate into Compost, Landfill and Recycling.\nTo train the model they ask that you place a service limit increase, which is something I have never needed to do outside of a work context. Living on the edge!\nUnfortunately these instructions do follow a similar pattern to my previous attempt, where there we a number of the sections involve commands where the SageMaker interface has changed. I ended up squeezing through with thankful backwards compatibility, allowing me to execute outdated commands.\nLuck was not with me everywhere though; when testing a set of images against the newly created SageMaker endpoint the provided Jupiter notebook commands were now unable to be executed against due to interface changes.\nLuckily a quick search resulted in finding this repo ZacksAmber/Trash-Sorter, where an updated version of the notebook contains changes to the non-working commands. This allowed me to slide through until the end. Thank you internet stranger!\nStep 2 - Deploy, now from SageMaker itself Like previously, we can now shift gears back to the DeepLens device to deploy our model. This time however, instead of deploying the static model from a S3 bucket, we can now reference a \u0026lsquo;Amazon SageMaker training job name\u0026rsquo; to supply our training model!\n The answer we all want to know; does it work? Well, yes. It\u0026rsquo;s not magic but it does work if you create a favourable environment similar to your input data environment. Looking at the input images I noticed a lot of the images were taken from an angle, somewhat top down.\nSo, what happens when you place a can of beans down and point the camera directly at it?\nEh, it\u0026rsquo;s correct but it\u0026rsquo;s not sure it\u0026rsquo;s recycling, which is not what we want.\nNow, say we take the camera angle into account this time and point the camera at the can in a similar fashion to our training data?\nWE HAVE A MATCH!\nLesson Learnt For myself the biggest take away was that the environment in which the training data was captured can effect how the model will interpret the field data later on. Seems obvious in hindsight, however I was confused why topdown angles were yielding the best results at first. This was until I checked out the training images and then it all made sense.\nAlso, for the record, apparently my phone is compost and my left hand is landfill. Who knew?\n","permalink":"http://dev.drentin.au/posts/compost-landfill-recycling/","summary":"What we all came here for\u0026hellip; SageMaker! I was excited to get stuck into the Advanced recipe - Build a custom ML model to sort trash as this started getting into the parts I wanted to know more about; how to get a basic model trained in SageMaker and then deploy it to the DeepLens device.\nStep 1 - Train! Luckily in this example they include a number of sample images, quite a decent set really with over 500 images in total, seperate into Compost, Landfill and Recycling.","title":"ML with a AWS DeepLens: Compost, Landfill or Recycling..?"},{"content":"Beginnings start here! Hello all and welcome to my first article around my attempts to create an Amazon SageMaker-based solution, focusing on image detection.\nI am participating in an initiative as part of my company\u0026rsquo;s AWS Community of Practice. The idea is inspired loosely by A Cloud Guru\u0026rsquo;s How to Build a Netflix Style Recommendation Engine with Amazon SageMaker Challenge and I have got my hands on a AWS DeepLens so I\u0026rsquo;m going to see what I can do with both of these to the best of my ability!\nSetup - Resurrect the DeepLens! I was given a AWS DeepLens in a sad state earlier this year with a goal - get the thing working again. On inspection it appeared someone had done something pretty horrific to it and installed a non-Deep Lens Ubuntu distribution on it. Or at least, they tried and failed terribly. I was luckily able to get my hands on a Factory Restore image for my 1.0 version and restore it to its former glory - of functioning!\nTrial 1 - The Fated AWS Example Initially I tried out using one of the AWS examples - amazon-sagemaker-object-detection-from-scratch.\nThis repo suffered from a common theme in this series - it was only written a mere two years ago and yet is already showing significant signs of ageing through the use of Python 2. It amazes me how fast these ecosystems are transforming sometimes - two years is a long time in the Python world it appears.\nI could not get this solution to deploy correctly no matter what I did. When running trigger_app.py I could not get past a timeout issue - I suspect a network connection was being blocked, somewhere, at some level. I wasn\u0026rsquo;t willing to debug any further so I shifted focus and moved onto something different - a AWS DeepLens Recipe!\nTrial 2 - Cooking with AWS With a renewed focus on figuring out how to interact correctly with the AWS DeepLens itself first I decide just to do the most basic AWS DeepLens example - Object Detection! This should be pretty easy right..?\nDeploying the project itself isn\u0026rsquo;t too hard, as you would hope for a Beginner example. You pretty much just had to deploy the detection framework to the DeepLens and let it do its thing. Plug-n-Play at it\u0026rsquo;s finest.\nIto-En is certified bottle-like to a relatively high percentage! It was a relief to see it was working and that I could now connect and get an output from the device.\nIt was time to turn things up and move onto to the next step - attempting an Intermediate example.\nTrial 3 - Pictionary with Alexa - What could go wrong? I chose the Building a Pictionary-style game with AWS DeepLens example as I\u0026rsquo;m hoping to involve text detection in my future solution, so drawing detection was right up my lane. This solution also involved a number of services - Lambda, Kinesis Data Streams, DeepLens, IoT and most interestingly\u0026hellip; Alexa.\nThe first step was probably the most problematic step - creating a Lambda function that used Python 2.7. As 2.7 is now a depreciated Lambda running environment in AWS you can no longer create these from the AWS Management Console. You can, however, repurpose existing Lambda functions that are using this depreciated Python version for new purposes. Luckily my failed initial trial ran a AWS CloudFormation template that created one of these Python 2.7 Lambda functions so I ripped out the insides of that function and repurposed it for this function. Best practise architects are probably throwing their hands up in dismay but I am not going to rewrite every piece of Python 2 to 3 every time I come across something that\u0026rsquo;s written in Python 2. I still don\u0026rsquo;t fully understand the differences myself so the task is not something I am ready for\u0026hellip; just yet.\nSetting up the Kinesis Data Stream was interesting - I have studied Kinesis to death over the last few years but I\u0026rsquo;ve never actually used it in something practical - certification examples do not count here. In this solution, however, we\u0026rsquo;re streaming the results computed on the DeepLens device via a AWS IoT MQTT queue through to the Kinesis Data Streams. Seeing the messages come through on the CloudWatch dashboard was cool once I got the solution running, and helpful in the end as I named my stream incorrectly and had to clarrify if anything was actually getting sent through.\nNext was facing off with Alexa - a fiend I had never even used myself previously, let alone logged into into the web interface. Setup was relatively easy, if not a bit confusing. It suffers from the old Amazon or AWS log in confusion - I was bashing away with my AWS credentials when it actually required my Amazon credentials. The interface is also a different beast to attend to - not bad, but definitely not AWS.\nI created a new Skill, dumped in the JSON examples to setup my Intents and pointed it to my ye olde Python 2.7 Lambda function. Upon building the Skill I was then able to test it out and\u0026hellip; it didn\u0026rsquo;t work. I had misnamed my Kinesis Data Stream which resulted in my Lambda function being unable to find a shard for a \u0026lsquo;RawDataStream\u0026rsquo; stream, returning null to Alexa. Initially a bit of a downer but once fixed we had a working skill!\nPlaying the game it will ask you to draw something from a predetermined list of objects in 4 seconds (news flash - I was unable to draw something in 4 seconds\u0026hellip; but it waits for you to reply so not all was lost). Looking at the DeepLens Project Output it will detect what you\u0026rsquo;ve drawn and send the guess via the IoT MQTT queue to the Kinesis Data Stream. When you tell Alexa you\u0026rsquo;re finished drawing it will invoke your Lambda function, which itself pulls data stored in your Kinesis Data Stream and inspects what your DeepLens suspected your drawing was of - if it matches up with what was asked you win!\nIf it doesn\u0026rsquo;t match up\u0026hellip; well, you lose this round.\nAnd forward, on to the next\u0026hellip; an Advanced recipe! Overall this process has been pretty enjoyable and all up I\u0026rsquo;m still sitting at $0.02 in my AWS Account. I\u0026rsquo;m not sure if the costs have actually come through yet but for a cost of 2c I\u0026rsquo;m pretty happy. Next up is moving onto the Advanced recipe - the DeepLens Trash Classiffication Recipe. This will involve Amazon SageMaker, which is ultimately what I\u0026rsquo;m here for. I\u0026rsquo;m pumped to see this in action!\n","permalink":"http://dev.drentin.au/posts/first-figure-out-how-to-do/","summary":"Beginnings start here! Hello all and welcome to my first article around my attempts to create an Amazon SageMaker-based solution, focusing on image detection.\nI am participating in an initiative as part of my company\u0026rsquo;s AWS Community of Practice. The idea is inspired loosely by A Cloud Guru\u0026rsquo;s How to Build a Netflix Style Recommendation Engine with Amazon SageMaker Challenge and I have got my hands on a AWS DeepLens so I\u0026rsquo;m going to see what I can do with both of these to the best of my ability!","title":"ML with a AWS DeepLens: First figure out how to do... anything!"}]